{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports, Configurables and Functions"
      ],
      "metadata": {
        "id": "Ha88VPilr9N8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dz12kix0qexB",
        "outputId": "2171a4db-09de-4d8f-8e05-210382fb4290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "# Install kagglehub\n",
        "!pip install kagglehub\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import kagglehub\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurable constants\n",
        "IMAGE_SIZE = (256, 256)      # Desired image size (e.g., 64x64, 256x256)\n",
        "REDUCTION_RATIO = 0.99       # Fraction of the dataset to keep\n",
        "BATCH_SIZE = 64              # Batch size for training and validation\n",
        "NUM_EPOCHS = 40              # Number of training epochs\n",
        "LEARNING_RATE = 0.0001       # Learning rate for the optimizer\n",
        "PATIENCE = 5                 # Patience for early stopping"
      ],
      "metadata": {
        "id": "5HE0joqnrFJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9r2TTWorH8n",
        "outputId": "eaf48965-db0a-4d7d-90bd-8652b0bc9396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the UTKFace dataset\n",
        "path = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
        "data_dir = os.path.join(path, \"UTKFace\")"
      ],
      "metadata": {
        "id": "AF6hjkiGrJYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the dataset by extracting image file paths and corresponding age labels.\n",
        "def preprocess_dataset(data_dir):\n",
        "    # Get list of image files\n",
        "    image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
        "    data = []\n",
        "\n",
        "    # Extract age from filename and create a DataFrame\n",
        "    for file in image_files:\n",
        "        try:\n",
        "            age = int(file.split(\"_\")[0])  # Filename format: age_gender_race_date.jpg\n",
        "            data.append({\"file_path\": os.path.join(data_dir, file), \"age\": age})\n",
        "        except ValueError:\n",
        "            print(f\"Skipping file: {file} (invalid format)\")\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "iYtW3meKrMj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the dataset size using stratified sampling based on age groups.\n",
        "def reduce_dataset(df, reduction_ratio, age_bins):\n",
        "    # Create age groups\n",
        "    df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=False, right=False)\n",
        "\n",
        "    # Filter out age groups with fewer than 2 samples\n",
        "    group_counts = df['age_group'].value_counts()\n",
        "    valid_groups = group_counts[group_counts >= 2].index\n",
        "    df = df[df['age_group'].isin(valid_groups)].reset_index(drop=True)\n",
        "\n",
        "    # Adjust reduction ratio if necessary\n",
        "    adjusted_ratio = max(reduction_ratio, 2 / len(df))\n",
        "\n",
        "    # Perform stratified sampling\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - adjusted_ratio, random_state=42)\n",
        "    for train_index, _ in sss.split(df, df['age_group']):\n",
        "        df_reduced = df.iloc[train_index].reset_index(drop=True)\n",
        "    return df_reduced"
      ],
      "metadata": {
        "id": "nCRqicW2rN5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the reduced dataset into training, validation, and test sets with stratification.\n",
        "def split_dataset(df_reduced):\n",
        "    # Ensure each age group has at least two members\n",
        "    group_counts = df_reduced['age_group'].value_counts()\n",
        "    valid_groups = group_counts[group_counts >= 2].index\n",
        "    df_reduced = df_reduced[df_reduced['age_group'].isin(valid_groups)].reset_index(drop=True)\n",
        "\n",
        "    # Split into training and test sets\n",
        "    train_df, test_df = train_test_split(\n",
        "        df_reduced, test_size=0.3, random_state=42, stratify=df_reduced['age_group'])\n",
        "\n",
        "    # Split the test set into validation and test sets\n",
        "    val_df, test_df = train_test_split(\n",
        "        test_df, test_size=0.5, random_state=42, stratify=test_df['age_group'])\n",
        "\n",
        "    # Drop the temporary 'age_group' column\n",
        "    train_df = train_df.drop(columns=['age_group'])\n",
        "    val_df = val_df.drop(columns=['age_group'])\n",
        "    test_df = test_df.drop(columns=['age_group'])\n",
        "\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "c4QoMcI3rQAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "df = preprocess_dataset(data_dir)\n",
        "print(f\"Total images: {len(df)}\")\n",
        "\n",
        "# Define age bins for stratification\n",
        "age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 117]\n",
        "\n",
        "# Reduce the dataset size\n",
        "df_reduced = reduce_dataset(df, REDUCTION_RATIO, age_bins)\n",
        "print(f\"Reduced dataset size: {len(df_reduced)}\")\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_df, val_df, test_df = split_dataset(df_reduced)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzMOPg9zrRu3",
        "outputId": "c6278963-28d8-411c-95d2-d9f2217de0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 23708\n",
            "Reduced dataset size: 23470\n",
            "Training set size: 16429\n",
            "Validation set size: 3520\n",
            "Test set size: 3521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations for training data with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # Normalization parameters\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Transformations for validation and test data without augmentation\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],    # Same normalization parameters\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "QEobC7lmrlqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Age Estimation CNN"
      ],
      "metadata": {
        "id": "kTc19aaXyaAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset for loading UTKFace images and their corresponding age labels.\n",
        "class UTKFaceDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image file path and age label\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image = Image.open(row[\"file_path\"]).convert(\"RGB\")\n",
        "        label = row[\"age\"]\n",
        "\n",
        "        # Apply transformations if any\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Return image and label as a tensor\n",
        "        return image, torch.tensor(label, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "y3K3Y6a729pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = UTKFaceDataset(train_df, transform=train_transform)\n",
        "val_dataset = UTKFaceDataset(val_df, transform=val_transform)\n",
        "test_dataset = UTKFaceDataset(test_df, transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                         num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ9gP7k_wLIH",
        "outputId": "dcc64636-bf57-4092-fc8e-099b2e7587e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A CNN model for age estimation from images.\n",
        "class AgeEstimationCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AgeEstimationCNN, self).__init__()\n",
        "        # Convolutional layers with batch normalization\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate the size of the input for the first fully connected layer\n",
        "        fc_input_size = 512 * (IMAGE_SIZE[0] // (2**5)) * (IMAGE_SIZE[1] // (2**5))\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        self.fc1 = nn.Linear(fc_input_size, 1024)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(1024, 1)  # Output layer for age regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolutional layers with ReLU activation and pooling\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Output size: H/2, W/2\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Output size: H/4, W/4\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Output size: H/8, W/8\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # Output size: H/16, W/16\n",
        "        x = self.pool(F.relu(self.bn5(self.conv5(x))))  # Output size: H/32, W/32\n",
        "\n",
        "        # Flatten the tensor for fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply fully connected layers with ReLU and dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)  # Output the estimated age\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "j9GHKWcfwScR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = AgeEstimationCNN().to(device)\n",
        "\n",
        "# Initialize weights of the model using appropriate initialization methods.\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "# Apply weight initialization\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "# Define loss function (Mean Squared Error for regression)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define optimizer (Adam optimizer)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler to adjust learning rate during training\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "KD_26c6cwYef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping parameters\n",
        "best_mae = float('inf')  # Initialize best Mean Absolute Error\n",
        "patience = PATIENCE      # Number of epochs to wait before early stopping\n",
        "trigger_times = 0        # Counter for early stopping"
      ],
      "metadata": {
        "id": "RxpAhubEyeB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Iterate over training data\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float().unsqueeze(1)  # Reshape labels\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_val_loss = 0.0\n",
        "    total_mae = 0.0  # Mean Absolute Error\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).float().unsqueeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            # Calculate Mean Absolute Error\n",
        "            mae = torch.abs(outputs - labels).mean().item()\n",
        "            total_mae += mae\n",
        "\n",
        "    # Calculate average validation loss and MAE\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    avg_mae = total_mae / len(val_loader)\n",
        "\n",
        "    # Print training and validation statistics\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
        "          f\"Val MAE: {avg_mae:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if avg_mae < best_mae:\n",
        "        best_mae = avg_mae\n",
        "        trigger_times = 0\n",
        "        # Save the best model weights\n",
        "        torch.save(model.state_dict(), 'age_estimation_model.pth')\n",
        "        print(\"Validation MAE decreased, saving model...\")\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        print(f\"Validation MAE did not improve. Trigger times: {trigger_times}\")\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5lLIsYGy7cT",
        "outputId": "becaacce-ecd8-4e9f-c450-112b66fff951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40, Train Loss: 263.5961, Val Loss: 166.1904, Val MAE: 10.0642\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 2/40, Train Loss: 173.4490, Val Loss: 133.7179, Val MAE: 8.6674\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 3/40, Train Loss: 144.9623, Val Loss: 125.6387, Val MAE: 8.4578\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 4/40, Train Loss: 132.0370, Val Loss: 106.2599, Val MAE: 7.6829\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 5/40, Train Loss: 121.5501, Val Loss: 97.4598, Val MAE: 7.2920\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 6/40, Train Loss: 114.4948, Val Loss: 111.1744, Val MAE: 7.7012\n",
            "Validation MAE did not improve. Trigger times: 1\n",
            "Epoch 7/40, Train Loss: 106.5586, Val Loss: 95.5599, Val MAE: 7.1966\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 8/40, Train Loss: 93.7656, Val Loss: 85.0309, Val MAE: 6.8025\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 9/40, Train Loss: 88.9150, Val Loss: 84.5422, Val MAE: 6.7085\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 10/40, Train Loss: 87.3234, Val Loss: 83.2014, Val MAE: 6.7015\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 11/40, Train Loss: 87.7934, Val Loss: 83.4960, Val MAE: 6.6592\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 12/40, Train Loss: 86.3176, Val Loss: 83.0085, Val MAE: 6.6812\n",
            "Validation MAE did not improve. Trigger times: 1\n",
            "Epoch 13/40, Train Loss: 85.9666, Val Loss: 81.3352, Val MAE: 6.6155\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 14/40, Train Loss: 84.8576, Val Loss: 81.9525, Val MAE: 6.5613\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 15/40, Train Loss: 82.5097, Val Loss: 80.5250, Val MAE: 6.5414\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 16/40, Train Loss: 82.4021, Val Loss: 80.4948, Val MAE: 6.5387\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 17/40, Train Loss: 82.0531, Val Loss: 80.5549, Val MAE: 6.5461\n",
            "Validation MAE did not improve. Trigger times: 1\n",
            "Epoch 18/40, Train Loss: 82.0581, Val Loss: 80.5811, Val MAE: 6.5339\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 19/40, Train Loss: 81.4206, Val Loss: 80.5394, Val MAE: 6.5451\n",
            "Validation MAE did not improve. Trigger times: 1\n",
            "Epoch 20/40, Train Loss: 82.9370, Val Loss: 80.6857, Val MAE: 6.5294\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 21/40, Train Loss: 81.5756, Val Loss: 80.7860, Val MAE: 6.5337\n",
            "Validation MAE did not improve. Trigger times: 1\n",
            "Epoch 22/40, Train Loss: 81.6290, Val Loss: 80.4424, Val MAE: 6.5286\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 23/40, Train Loss: 80.9133, Val Loss: 80.3012, Val MAE: 6.5336\n",
            "Validation MAE did not improve. Trigger times: 1\n",
            "Epoch 24/40, Train Loss: 81.5906, Val Loss: 80.4504, Val MAE: 6.5295\n",
            "Validation MAE did not improve. Trigger times: 2\n",
            "Epoch 25/40, Train Loss: 79.5211, Val Loss: 80.3240, Val MAE: 6.5295\n",
            "Validation MAE did not improve. Trigger times: 3\n",
            "Epoch 26/40, Train Loss: 82.6886, Val Loss: 80.3828, Val MAE: 6.5237\n",
            "Validation MAE decreased, saving model...\n",
            "Epoch 27/40, Train Loss: 82.1676, Val Loss: 80.1978, Val MAE: 6.5267\n",
            "Validation MAE did not improve. Trigger times: 1\n",
            "Epoch 28/40, Train Loss: 81.2336, Val Loss: 80.1855, Val MAE: 6.5284\n",
            "Validation MAE did not improve. Trigger times: 2\n",
            "Epoch 29/40, Train Loss: 81.2387, Val Loss: 80.3032, Val MAE: 6.5317\n",
            "Validation MAE did not improve. Trigger times: 3\n",
            "Epoch 30/40, Train Loss: 81.5110, Val Loss: 80.5399, Val MAE: 6.5244\n",
            "Validation MAE did not improve. Trigger times: 4\n",
            "Epoch 31/40, Train Loss: 80.6055, Val Loss: 80.1633, Val MAE: 6.5305\n",
            "Validation MAE did not improve. Trigger times: 5\n",
            "Early stopping!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load('age_estimation_model.pth'))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "total_mae_test = 0.0\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float().unsqueeze(1)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Mean Absolute Error\n",
        "        mae = torch.abs(outputs - labels).mean().item()\n",
        "        total_mae_test += mae\n",
        "\n",
        "# Print the final test MAE\n",
        "print(f\"Test Mean Absolute Error (MAE): {total_mae_test / len(test_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BibXMgRy8WQ",
        "outputId": "0b611003-6b06-4fda-d3c4-382a92db18dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-c4859a848353>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('age_estimation_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Mean Absolute Error (MAE): 6.5153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet Transfer Learning"
      ],
      "metadata": {
        "id": "WNib7LllhXON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub\n",
        "\n",
        "# ================================================\n",
        "# Age Estimation using ResNet18 with Transfer Learning\n",
        "# ================================================\n",
        "\n",
        "# ----------------------------\n",
        "# Step 1: Import Libraries\n",
        "# ----------------------------\n",
        "\n",
        "import os\n",
        "import kagglehub\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from torchvision import transforms, models\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.optim as optim\n",
        "\n",
        "# ----------------------------\n",
        "# Step 2: Define Configurable Constants\n",
        "# ----------------------------\n",
        "\n",
        "# Image size expected by ResNet18\n",
        "IMAGE_SIZE = (224, 224)\n",
        "\n",
        "# Device configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Hyperparameters\n",
        "REDUCTION_RATIO = 1.0          # Use full dataset\n",
        "BATCH_SIZE = 64                # Batch size for training and validation\n",
        "NUM_EPOCHS = 10                # Increased number of training epochs\n",
        "LEARNING_RATE = 1e-4           # Learning rate for the optimizer\n",
        "PATIENCE = 10                  # Increased patience for early stopping\n",
        "\n",
        "# ----------------------------\n",
        "# Step 3: Download and Preprocess the Dataset\n",
        "# ----------------------------\n",
        "\n",
        "# Download the UTKFace dataset using kagglehub\n",
        "path = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
        "data_dir = os.path.join(path, \"UTKFace\")\n",
        "\n",
        "# Function to preprocess dataset: extract image paths and age labels\n",
        "def preprocess_dataset(data_dir):\n",
        "    # List all image files in the directory\n",
        "    image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
        "    data = []\n",
        "\n",
        "    # Extract age from filename and create a DataFrame\n",
        "    for file in image_files:\n",
        "        try:\n",
        "            age = int(file.split(\"_\")[0])  # Filename format: age_gender_race_date.jpg\n",
        "            data.append({\"file_path\": os.path.join(data_dir, file), \"age\": age})\n",
        "        except ValueError:\n",
        "            print(f\"Skipping file: {file} (invalid format)\")\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "df = preprocess_dataset(data_dir)\n",
        "print(f\"Total images: {len(df)}\")\n",
        "\n",
        "# Function to reduce dataset size using stratified sampling based on age groups\n",
        "def reduce_dataset(df, reduction_ratio, age_bins):\n",
        "    if reduction_ratio < 1.0:\n",
        "        # Create age groups\n",
        "        df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=False, right=False)\n",
        "\n",
        "        # Filter out age groups with fewer than 2 samples\n",
        "        group_counts = df['age_group'].value_counts()\n",
        "        valid_groups = group_counts[group_counts >= 2].index\n",
        "        df = df[df['age_group'].isin(valid_groups)].reset_index(drop=True)\n",
        "\n",
        "        # Perform stratified sampling\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - reduction_ratio, random_state=42)\n",
        "        for train_index, _ in sss.split(df, df['age_group']):\n",
        "            df_reduced = df.iloc[train_index].reset_index(drop=True)\n",
        "        return df_reduced\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "# Define age bins for stratification\n",
        "age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 117]\n",
        "\n",
        "# Reduce the dataset size\n",
        "df_reduced = reduce_dataset(df, REDUCTION_RATIO, age_bins)\n",
        "print(f\"Reduced dataset size: {len(df_reduced)}\")\n",
        "\n",
        "# Function to split the dataset into training, validation, and test sets\n",
        "def split_dataset(df_reduced, age_bins):\n",
        "    # Create age groups\n",
        "    df_reduced['age_group'] = pd.cut(df_reduced['age'], bins=age_bins, labels=False, right=False)\n",
        "\n",
        "    # Split into training and temp (validation + test)\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df_reduced, test_size=0.3, random_state=42, stratify=df_reduced['age_group'])\n",
        "\n",
        "    # Split temp into validation and test\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df, test_size=0.5, random_state=42, stratify=temp_df['age_group'])\n",
        "\n",
        "    # Drop the temporary 'age_group' column\n",
        "    train_df = train_df.drop(columns=['age_group'])\n",
        "    val_df = val_df.drop(columns=['age_group'])\n",
        "    test_df = test_df.drop(columns=['age_group'])\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# Split the dataset\n",
        "train_df, val_df, test_df = split_dataset(df_reduced, age_bins)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Step 4: Define Data Augmentation and Transformation\n",
        "# ----------------------------\n",
        "\n",
        "# Define data augmentation and normalization for training\n",
        "# Just normalization for validation and testing\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(IMAGE_SIZE[0]),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(20),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],   # ImageNet mean\n",
        "                             [0.229, 0.224, 0.225])   # ImageNet std\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(IMAGE_SIZE[0]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(IMAGE_SIZE[0]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Step 5: Create Custom Dataset Class\n",
        "# ----------------------------\n",
        "\n",
        "class UTKFaceDataset(Dataset):\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.dataframe = dataframe.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image = Image.open(row[\"file_path\"]).convert(\"RGB\")\n",
        "        age = row[\"age\"]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(age, dtype=torch.float32)\n",
        "\n",
        "# ----------------------------\n",
        "# Step 6: Create DataLoaders\n",
        "# ----------------------------\n",
        "\n",
        "# Create datasets\n",
        "image_datasets = {\n",
        "    'train': UTKFaceDataset(train_df, transform=data_transforms['train']),\n",
        "    'val': UTKFaceDataset(val_df, transform=data_transforms['val']),\n",
        "    'test': UTKFaceDataset(test_df, transform=data_transforms['test']),\n",
        "}\n",
        "\n",
        "# Create DataLoaders\n",
        "dataloaders = {\n",
        "    'train': DataLoader(image_datasets['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True),\n",
        "    'val': DataLoader(image_datasets['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True),\n",
        "    'test': DataLoader(image_datasets['test'], batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True),\n",
        "}\n",
        "\n",
        "# ----------------------------\n",
        "# Step 7: Define the Model\n",
        "# ----------------------------\n",
        "\n",
        "# Load the pretrained ResNet18 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze layer4 and fc for fine-tuning\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Modify the final fully connected layer for regression\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 1)\n",
        ")\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# ----------------------------\n",
        "# Step 8: Define Loss Function, Optimizer, and Scheduler\n",
        "# ----------------------------\n",
        "\n",
        "# Use Mean Squared Error Loss for regression\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Update the optimizer to include parameters that are being fine-tuned\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "\n",
        "# Define learning rate scheduler to reduce LR on plateau\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# ----------------------------\n",
        "# Step 9: Training and Validation Loop\n",
        "# ----------------------------\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=30, patience=10):\n",
        "    best_mae = float('inf')\n",
        "    best_model_wts = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_mae = 0.0\n",
        "            running_samples = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE).unsqueeze(1)  # Shape: [batch_size, 1]\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    mae = torch.abs(outputs - labels).sum().item()\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_mae += mae\n",
        "                running_samples += inputs.size(0)\n",
        "\n",
        "            epoch_loss = running_loss / running_samples\n",
        "            epoch_mae = running_mae / running_samples\n",
        "\n",
        "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} MAE: {epoch_mae:.4f}\")\n",
        "\n",
        "            # Step the scheduler based on validation loss\n",
        "            if phase == 'val':\n",
        "                scheduler.step(epoch_loss)\n",
        "\n",
        "                # Early stopping\n",
        "                if epoch_mae < best_mae:\n",
        "                    best_mae = epoch_mae\n",
        "                    best_model_wts = model.state_dict()\n",
        "                    patience_counter = 0\n",
        "                    torch.save(best_model_wts, 'best_resnet18_age_estimation.pth')\n",
        "                    print(\"Validation MAE improved. Model saved.\")\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    print(f\"No improvement in Validation MAE. Patience counter: {patience_counter}/{patience}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Check early stopping condition\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Best Validation MAE: {best_mae:.4f}\")\n",
        "\n",
        "    # Load best model weights\n",
        "    if best_model_wts is not None:\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "    return model\n",
        "\n",
        "# ----------------------------\n",
        "# Step 10: Train the Model\n",
        "# ----------------------------\n",
        "\n",
        "# Train the model\n",
        "model = train_model(model, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS, patience=PATIENCE)\n",
        "\n",
        "# ----------------------------\n",
        "# Step 11: Evaluate the Model on Test Set\n",
        "# ----------------------------\n",
        "\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    total_mae = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE).unsqueeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            mae = torch.abs(outputs - labels).sum().item()\n",
        "\n",
        "            total_mae += mae\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    final_mae = total_mae / total_samples\n",
        "    return final_mae\n",
        "\n",
        "# Evaluate on test set\n",
        "test_mae = evaluate_model(model, dataloaders['test'])\n",
        "print(f\"Final Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Step 12: Visualize Predictions\n",
        "# ----------------------------\n",
        "\n",
        "def visualize_predictions(model, dataloader, num_samples=100):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE).unsqueeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "            if len(predictions) >= num_samples:\n",
        "                break\n",
        "\n",
        "    predictions = np.array(predictions[:num_samples]).flatten()\n",
        "    actuals = np.array(actuals[:num_samples]).flatten()\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(actuals, predictions, alpha=0.6)\n",
        "    plt.plot([0, 120], [0, 120], 'r--')  # Diagonal line\n",
        "    plt.xlabel('Actual Age')\n",
        "    plt.ylabel('Predicted Age')\n",
        "    plt.title('Actual vs. Predicted Age')\n",
        "    plt.xlim(0, 120)\n",
        "    plt.ylim(0, 120)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Visualize predictions\n",
        "visualize_predictions(model, dataloaders['test'])\n",
        "\n",
        "# ----------------------------\n",
        "# Step 13: Save the Final Model\n",
        "# ----------------------------\n",
        "\n",
        "# Save the model architecture and weights\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, 'resnet18_age_estimation_final.pth')\n",
        "\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "Rg10LDiAhZ3N",
        "outputId": "5e2d36d5-b809-4660-9b01-43aa5a4da913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n",
            "Using device: cuda\n",
            "Total images: 23708\n",
            "Reduced dataset size: 23708\n",
            "Training set size: 16595\n",
            "Validation set size: 3556\n",
            "Test set size: 3557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 166MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-b61d3ceb1075>\u001b[0m in \u001b[0;36m<cell line: 328>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATIENCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-b61d3ceb1075>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs, patience)\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                     \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;31m# Backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gender Classificiation"
      ],
      "metadata": {
        "id": "ePc1cRlE7D2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install kagglehub\n",
        "!pip install kagglehub\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import kagglehub\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler"
      ],
      "metadata": {
        "id": "0JUhEC6Q7Swk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "outputId": "ffaf3b2a-2d0a-4502-9d67-9c664f1b861c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.12.14)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "<built-in function isinstance> returned a result with an exception set",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'pandas' has no attribute '_pandas_datetime_CAPI' (most likely due to a circular import)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e799eda1079e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mArrowDtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m from pandas.core.groupby import (\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mGrouper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from pandas.core.groupby.generic import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mDataFrameGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mNamedAgg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mSeriesGroupBy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m from pandas.core.groupby import (\n\u001b[1;32m     70\u001b[0m     \u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0msanitize_masked_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m )\n\u001b[0;32m--> 149\u001b[0;31m from pandas.core.generic import (\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0mNDFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0mmake_doc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m from pandas.core import (\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0malgorithms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0marraylike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mlength_of_indexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m )\n\u001b[0;32m---> 79\u001b[0;31m from pandas.core.indexes.api import (\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mMultiIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_sort\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m from pandas.core.indexes.base import (\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0m_new_Index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m from pandas._libs import (\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0malgos\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlibalgos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36minit pandas._libs.index\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: <built-in function isinstance> returned a result with an exception set"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurable constants\n",
        "IMAGE_SIZE = (256, 256)      # Desired image size (e.g., 64x64, 256x256)\n",
        "REDUCTION_RATIO = 0.41       # Fraction of the dataset to keep\n",
        "BATCH_SIZE = 64              # Batch size for training and validation\n",
        "NUM_EPOCHS = 5              # Number of training epochs\n",
        "LEARNING_RATE = 0.0001       # Learning rate for the optimizer\n",
        "PATIENCE = 5                 # Patience for early stopping"
      ],
      "metadata": {
        "id": "IYPG6MoH7WNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "Rnd6cM3T7YLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the UTKFace dataset\n",
        "path = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
        "data_dir = os.path.join(path, \"UTKFace\")"
      ],
      "metadata": {
        "id": "oZ5vRHrw7Zg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the dataset by extracting image file paths and corresponding gender labels.\n",
        "def preprocess_dataset(data_dir):\n",
        "    # Get list of image files\n",
        "    image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
        "    data = []\n",
        "\n",
        "    # Extract gender from filename and create a DataFrame\n",
        "    for file in image_files:\n",
        "        try:\n",
        "            gender = int(file.split(\"_\")[1])  # Filename format: age_gender_race_date.jpg\n",
        "            data.append({\"file_path\": os.path.join(data_dir, file), \"gender\": gender})\n",
        "        except ValueError:\n",
        "            print(f\"Skipping file: {file} (invalid format)\")\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "Hz6069F37a0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the dataset size using stratified sampling based on gender groups.\n",
        "def reduce_dataset(df, reduction_ratio, gender_bins):\n",
        "    # Create gender groups\n",
        "    df['gender_group'] = pd.cut(df['gender'], bins=gender_bins, labels=False, right=False)\n",
        "\n",
        "    # Filter out gender groups with fewer than 2 samples\n",
        "    group_counts = df['gender_group'].value_counts()\n",
        "    valid_groups = group_counts[group_counts >= 2].index\n",
        "    df = df[df['gender_group'].isin(valid_groups)].reset_index(drop=True)\n",
        "\n",
        "    # Adjust reduction ratio if necessary\n",
        "    adjusted_ratio = max(reduction_ratio, 2 / len(df))\n",
        "\n",
        "    # Perform stratified sampling\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - adjusted_ratio, random_state=42)\n",
        "    for train_index, _ in sss.split(df, df['gender_group']):\n",
        "        df_reduced = df.iloc[train_index].reset_index(drop=True)\n",
        "    return df_reduced"
      ],
      "metadata": {
        "id": "H-semcph7bSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the reduced dataset into training, validation, and test sets with stratification.\n",
        "def split_dataset(df_reduced):\n",
        "    # Ensure each gender group has at least two members\n",
        "    group_counts = df_reduced['gender_group'].value_counts()\n",
        "    valid_groups = group_counts[group_counts >= 2].index\n",
        "    df_reduced = df_reduced[df_reduced['gender_group'].isin(valid_groups)].reset_index(drop=True)\n",
        "\n",
        "    # Split into training and test sets\n",
        "    train_df, test_df = train_test_split(\n",
        "        df_reduced, test_size=0.3, random_state=42, stratify=df_reduced['gender_group'])\n",
        "\n",
        "    # Split the test set into validation and test sets\n",
        "    val_df, test_df = train_test_split(\n",
        "        test_df, test_size=0.5, random_state=42, stratify=test_df['gender_group'])\n",
        "\n",
        "    # Drop the temporary 'gender_group' column\n",
        "    train_df = train_df.drop(columns=['gender_group'])\n",
        "    val_df = val_df.drop(columns=['gender_group'])\n",
        "    test_df = test_df.drop(columns=['gender_group'])\n",
        "\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "TsqKuMS27cg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset\n",
        "df = preprocess_dataset(data_dir)\n",
        "print(f\"Total images: {len(df)}\")\n",
        "\n",
        "# Define gender bins for stratification\n",
        "gender_bins = [0, 1]\n",
        "\n",
        "# Reduce the dataset size\n",
        "df_reduced = reduce_dataset(df, REDUCTION_RATIO, gender_bins)\n",
        "print(f\"Reduced dataset size: {len(df_reduced)}\")\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_df, val_df, test_df = split_dataset(df_reduced)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "-C-gD1Ec7dnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations for training data with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # Normalization parameters\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Transformations for validation and test data without augmentation\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],    # Same normalization parameters\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "lLlHCUst7e-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = UTKFaceDataset(train_df, transform=train_transform)\n",
        "val_dataset = UTKFaceDataset(val_df, transform=val_transform)\n",
        "test_dataset = UTKFaceDataset(test_df, transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                         num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "bONrGkHu7hy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A CNN model for gender estimation from images.\n",
        "class ImprovedGenderEstimationCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedGenderEstimationCNN, self).__init__()\n",
        "        # Convolutional layers with batch normalization\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate the size of the input for the first fully connected layer\n",
        "        fc_input_size = 512 * (IMAGE_SIZE[0] // (2**5)) * (IMAGE_SIZE[1] // (2**5))\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        self.fc1 = nn.Linear(fc_input_size, 1024)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(1024, 1)  # Output layer for gender regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply convolutional layers with ReLU activation and pooling\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # Output size: H/2, W/2\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # Output size: H/4, W/4\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # Output size: H/8, W/8\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # Output size: H/16, W/16\n",
        "        x = self.pool(F.relu(self.bn5(self.conv5(x))))  # Output size: H/32, W/32\n",
        "\n",
        "        # Flatten the tensor for fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Apply fully connected layers with ReLU and dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)  # Output the estimated gender\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Jsp4zuuj7jNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = ImprovedGenderEstimationCNN().to(device)\n",
        "\n",
        "# Initialize weights of the model using appropriate initialization methods.\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "# Apply weight initialization\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "# Define loss function (Mean Squared Error for regression)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Define optimizer (Adam optimizer)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler to adjust learning rate during training\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "uiHcqTDn7lU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()  # Set model to training mode\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Iterate over training data\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float().unsqueeze(1)  # Reshape labels\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_val_loss = 0.0\n",
        "    total_mae = 0.0  # Mean Absolute Error\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).float().unsqueeze(1)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            # Calculate Mean Absolute Error\n",
        "            mae = torch.abs(outputs - labels).mean().item()\n",
        "            total_mae += mae\n",
        "\n",
        "    # Calculate average validation loss and MAE\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    avg_mae = total_mae / len(val_loader)\n",
        "\n",
        "    # Print training and validation statistics\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}, \"\n",
        "          f\"Val MAE: {avg_mae:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if avg_mae < best_mae:\n",
        "        best_mae = avg_mae\n",
        "        trigger_times = 0\n",
        "        # Save the best model weights\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(\"Validation MAE decreased, saving model...\")\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        print(f\"Validation MAE did not improve. Trigger times: {trigger_times}\")\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break"
      ],
      "metadata": {
        "id": "HyKJBuT37mCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "total_mae_test = 0.0\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).float().unsqueeze(1)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Mean Absolute Error\n",
        "        mae = torch.abs(outputs - labels).mean().item()\n",
        "        total_mae_test += mae\n",
        "\n",
        "# Print the final test MAE\n",
        "print(f\"Test Mean Absolute Error (MAE): {total_mae_test / len(test_loader):.4f}\")"
      ],
      "metadata": {
        "id": "j-32cvfZ7qcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fixed Gender Classification"
      ],
      "metadata": {
        "id": "2nKHWDRv_R1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install kagglehub if not already installed\n",
        "!pip install kagglehub\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import kagglehub\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "# Configurable constants\n",
        "IMAGE_SIZE = (256, 256)      # Desired image size (e.g., 64x64, 256x256)\n",
        "REDUCTION_RATIO = 0.99       # Fraction of the dataset to keep\n",
        "BATCH_SIZE = 64              # Batch size for training and validation\n",
        "NUM_EPOCHS = 20              # Number of training epochs\n",
        "LEARNING_RATE = 0.0001       # Learning rate for the optimizer\n",
        "PATIENCE = 5                 # Patience for early stopping\n",
        "\n",
        "# Verify GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Download the UTKFace dataset\n",
        "path = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
        "data_dir = os.path.join(path, \"UTKFace\")\n",
        "\n",
        "# Preprocess the dataset by extracting image file paths and corresponding labels.\n",
        "def preprocess_dataset(data_dir):\n",
        "    # Get list of image files\n",
        "    image_files = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n",
        "    data = []\n",
        "\n",
        "    # Filename format: age_gender_race_date.jpg\n",
        "    # According to UTKFace: gender=0 is male, gender=1 is female.\n",
        "    for file in image_files:\n",
        "        try:\n",
        "            parts = file.split(\"_\")\n",
        "            age = int(parts[0])\n",
        "            gender = int(parts[1])  # 0=male, 1=female\n",
        "            data.append({\"file_path\": os.path.join(data_dir, file), \"age\": age, \"gender\": gender})\n",
        "        except ValueError:\n",
        "            print(f\"Skipping file: {file} (invalid format)\")\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Reduce the dataset size using stratified sampling based on age groups.\n",
        "def reduce_dataset(df, reduction_ratio, age_bins):\n",
        "    # Create age groups\n",
        "    df['age_group'] = pd.cut(df['age'], bins=age_bins, labels=False, right=False)\n",
        "\n",
        "    # Filter out age groups with fewer than 2 samples\n",
        "    group_counts = df['age_group'].value_counts()\n",
        "    valid_groups = group_counts[group_counts >= 2].index\n",
        "    df = df[df['age_group'].isin(valid_groups)].reset_index(drop=True)\n",
        "\n",
        "    # Adjust reduction ratio if necessary\n",
        "    adjusted_ratio = max(reduction_ratio, 2 / len(df))\n",
        "\n",
        "    # Perform stratified sampling on age_group\n",
        "    sss = StratifiedShuffleSplit(n_splits=1, test_size=1 - adjusted_ratio, random_state=42)\n",
        "    for train_index, _ in sss.split(df, df['age_group']):\n",
        "        df_reduced = df.iloc[train_index].reset_index(drop=True)\n",
        "    return df_reduced\n",
        "\n",
        "# Split the reduced dataset into training, validation, and test sets with stratification.\n",
        "def split_dataset(df_reduced):\n",
        "    # Ensure each age group has at least two members\n",
        "    group_counts = df_reduced['age_group'].value_counts()\n",
        "    valid_groups = group_counts[group_counts >= 2].index\n",
        "    df_reduced = df_reduced[df_reduced['age_group'].isin(valid_groups)].reset_index(drop=True)\n",
        "\n",
        "    # Split into training and test sets stratified by age_group\n",
        "    train_df, test_df = train_test_split(\n",
        "        df_reduced, test_size=0.3, random_state=42, stratify=df_reduced['age_group'])\n",
        "\n",
        "    # Split the test set into validation and test sets\n",
        "    val_df, test_df = train_test_split(\n",
        "        test_df, test_size=0.5, random_state=42, stratify=test_df['age_group'])\n",
        "\n",
        "    # Drop the temporary 'age_group' column\n",
        "    train_df = train_df.drop(columns=['age_group'])\n",
        "    val_df = val_df.drop(columns=['age_group'])\n",
        "    test_df = test_df.drop(columns=['age_group'])\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "df = preprocess_dataset(data_dir)\n",
        "print(f\"Total images: {len(df)}\")\n",
        "\n",
        "# Define age bins for stratification (example: 0-10,10-20,...,90-100)\n",
        "age_bins = range(0, 101, 10)\n",
        "\n",
        "# Reduce the dataset size\n",
        "df_reduced = reduce_dataset(df, REDUCTION_RATIO, age_bins)\n",
        "print(f\"Reduced dataset size: {len(df_reduced)}\")\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_df, val_df, test_df = split_dataset(df_reduced)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")\n",
        "\n",
        "class UTKFaceDataset(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = Image.open(row['file_path']).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        # Gender: 0=male, 1=female\n",
        "        label = row['gender']\n",
        "        return image, label\n",
        "\n",
        "# Transformations for training data with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Transformations for validation and test data without augmentation\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = UTKFaceDataset(train_df, transform=train_transform)\n",
        "val_dataset = UTKFaceDataset(val_df, transform=val_transform)\n",
        "test_dataset = UTKFaceDataset(test_df, transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# A CNN model for gender classification\n",
        "class ImprovedGenderEstimationCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedGenderEstimationCNN, self).__init__()\n",
        "        # Convolutional layers with batch normalization\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate the size of the input for the first fully connected layer\n",
        "        fc_input_size = 512 * (IMAGE_SIZE[0] // (2**5)) * (IMAGE_SIZE[1] // (2**5))\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        self.fc1 = nn.Linear(fc_input_size, 1024)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        # Output layer for binary classification (0=male, 1=female)\n",
        "        self.fc2 = nn.Linear(1024, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)  # Logits for binary classification\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = ImprovedGenderEstimationCNN().to(device)\n",
        "\n",
        "# Initialize weights\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "    elif isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "model.apply(initialize_weights)\n",
        "\n",
        "# For binary classification, use BCEWithLogitsLoss\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "trigger_times = 0\n",
        "patience = PATIENCE\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute training accuracy\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        preds = (probs > 0.5).float()\n",
        "        correct = (preds == labels.float().unsqueeze(1)).sum().item()\n",
        "        total_correct += correct\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_acc = total_correct / total_samples\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    total_val_correct = 0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels.float().unsqueeze(1))\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            preds = (probs > 0.5).float()\n",
        "            correct = (preds == labels.float().unsqueeze(1)).sum().item()\n",
        "            total_val_correct += correct\n",
        "            val_samples += labels.size(0)\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_acc = total_val_correct / val_samples\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping based on validation loss\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        trigger_times = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(\"Validation loss decreased, saving model...\")\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        print(f\"Validation loss did not improve. Trigger times: {trigger_times}\")\n",
        "        if trigger_times >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Test evaluation\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        preds = (probs > 0.5).float()\n",
        "        correct = (preds == labels.float().unsqueeze(1)).sum().item()\n",
        "        test_correct += correct\n",
        "        test_samples += labels.size(0)\n",
        "\n",
        "test_acc = test_correct / test_samples\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blhPrqTT_Vr4",
        "outputId": "0a6871a5-3821-4118-99f0-40ceb8f6bdbe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.12.14)\n",
            "Using device: cuda\n",
            "Total images: 23708\n",
            "Reduced dataset size: 23439\n",
            "Training set size: 16407\n",
            "Validation set size: 3516\n",
            "Test set size: 3516\n",
            "Epoch 1/20, Train Loss: 0.7510, Train Acc: 0.7103, Val Loss: 0.4348, Val Acc: 0.8100\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 2/20, Train Loss: 0.4120, Train Acc: 0.8152, Val Loss: 0.3519, Val Acc: 0.8521\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 3/20, Train Loss: 0.3543, Train Acc: 0.8403, Val Loss: 0.3131, Val Acc: 0.8555\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 4/20, Train Loss: 0.3201, Train Acc: 0.8595, Val Loss: 0.2874, Val Acc: 0.8706\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 5/20, Train Loss: 0.2972, Train Acc: 0.8704, Val Loss: 0.2701, Val Acc: 0.8786\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 6/20, Train Loss: 0.2817, Train Acc: 0.8782, Val Loss: 0.2696, Val Acc: 0.8817\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 7/20, Train Loss: 0.2718, Train Acc: 0.8835, Val Loss: 0.2774, Val Acc: 0.8788\n",
            "Validation loss did not improve. Trigger times: 1\n",
            "Epoch 8/20, Train Loss: 0.2397, Train Acc: 0.9003, Val Loss: 0.2451, Val Acc: 0.8959\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 9/20, Train Loss: 0.2338, Train Acc: 0.9020, Val Loss: 0.2410, Val Acc: 0.9002\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 10/20, Train Loss: 0.2274, Train Acc: 0.9056, Val Loss: 0.2384, Val Acc: 0.9022\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 11/20, Train Loss: 0.2261, Train Acc: 0.9064, Val Loss: 0.2377, Val Acc: 0.9013\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 12/20, Train Loss: 0.2218, Train Acc: 0.9095, Val Loss: 0.2389, Val Acc: 0.9024\n",
            "Validation loss did not improve. Trigger times: 1\n",
            "Epoch 13/20, Train Loss: 0.2244, Train Acc: 0.9080, Val Loss: 0.2355, Val Acc: 0.9027\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 14/20, Train Loss: 0.2213, Train Acc: 0.9106, Val Loss: 0.2331, Val Acc: 0.9024\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 15/20, Train Loss: 0.2152, Train Acc: 0.9116, Val Loss: 0.2331, Val Acc: 0.9027\n",
            "Validation loss did not improve. Trigger times: 1\n",
            "Epoch 16/20, Train Loss: 0.2133, Train Acc: 0.9131, Val Loss: 0.2331, Val Acc: 0.9019\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 17/20, Train Loss: 0.2164, Train Acc: 0.9139, Val Loss: 0.2327, Val Acc: 0.9067\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 18/20, Train Loss: 0.2161, Train Acc: 0.9107, Val Loss: 0.2326, Val Acc: 0.9059\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 19/20, Train Loss: 0.2159, Train Acc: 0.9116, Val Loss: 0.2322, Val Acc: 0.9044\n",
            "Validation loss decreased, saving model...\n",
            "Epoch 20/20, Train Loss: 0.2151, Train Acc: 0.9139, Val Loss: 0.2317, Val Acc: 0.9050\n",
            "Validation loss decreased, saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-cbc5117a0611>:301: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Live Example"
      ],
      "metadata": {
        "id": "b4jYTJTzeLtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Copy the class definition here:\n",
        "class AgeEstimationCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AgeEstimationCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(512)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Adjust this if your IMAGE_SIZE changed, but from the code you provided it was (256,256)\n",
        "        fc_input_size = 512 * (256 // (2**5)) * (256 // (2**5))\n",
        "\n",
        "        self.fc1 = nn.Linear(fc_input_size, 1024)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(1024, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Load model\n",
        "model = AgeEstimationCNN()\n",
        "model.load_state_dict(torch.load('age_estimation_model.pth', map_location='cpu'))\n",
        "model.eval()\n",
        "\n",
        "# Define transforms (must match training)\n",
        "input_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load and preprocess the input image\n",
        "image = Image.open('picture.jpg').convert('RGB')\n",
        "input_tensor = input_transforms(image).unsqueeze(0)\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)\n",
        "\n",
        "predicted_age = output.item()\n",
        "print(\"Predicted Age:\", predicted_age)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHa7wcU7ePzI",
        "outputId": "414a58bc-4539-4692-ebb6-68a277593c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-a677927958bd>:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('age_estimation_model.pth', map_location='cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Age: 25.045183181762695\n"
          ]
        }
      ]
    }
  ]
}